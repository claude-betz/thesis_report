\chapter{Implementation}\label{chapter_implementation}
This Chapter details the implementation of the relevant software modules of the
overall MT System. The work is firmly grounded within the Theoretical Framework
established in Chapter~\ref{chapter_theoretical_framework} and the system design
developed in Chapter~\ref{chapter_design}. The implementations documented here
are subsequently tested in Chapter~\ref{chapter_results}.

The focus of this Chapter is in the presentation of the algorithmic
implementation, independent of class structure. The reason behind this is so
that the parallels to the underlying theory in
Chapter~\ref{chapter_theoretical_framework} can better be appreciated.

The only differences between the class-based implementations according to the
UML Diagram in Figure~\ref{fig:design_uml} and those presented here, is that the
class functions are not procedurally implemented. The procedural implementation
was presented here to enhance clarity and reproducibility. The class based
implementations can be seen in Appendix~\ref{appendix_implementations}.

\section{Template Matching Tracker}\label{implementation_tm}
This Section details the implementation of the two variants of template tracker
described in Section~\ref{theoretical_framework_tm}. The template trackers were
implemented using the highly optimized $\text{openCV}_{TM}$ template matching
functions, as preliminary naive implementations would did meet the speed requirements to
tracking and research into optimization was deemed beyond the scope of this
study.

\subsection{Simple template matching tracker}

\subsection{Adaptive template matching tracker}


\section{Colour Co-occurrence Histogram Detector}\label{implementation_ch}
The implementation of the CH-Detector follows directly from the algorithm
description in Section~\ref{theoretical_framework_ch}.

\subsection{Computing model and candidate CCHs}
The model and candidate CCH's are computed by the \textit{get\_model\_CH
(template,nc,nd)} and \textit{get\_candidate\_CH (kmeans,template,nc,nd)}
functions respectively.

The model's euclidean RGB colour-space is quantized by use of the kmeans
clustering algorithm and the colour co-occurrence histogram computed using these
quantization levels. The kmeans centroids are returned and subsequently used to
quantize candidates when computing their CCH are shown in
Listings~\ref{lst:chmod} and~\ref{lst:chcand}.

\begin{lstlisting}[language=Python, caption={Computing model CH}, captionpos=b, label={lst:chmod}]
def get_model_CH(template, nc, nd):
    """computes overall CH along vertical and horizontal axes"""
    # step1: quantize rgb colourspace according to nc
    kmeans, q_labels2d = RGB_quantize_model(template, nc)
    
    # step2: compute vertical and horizontal CH's
    CH_v  = CH_vertical(q_labels2d, nc, nd) # compute vertical CH
    CH_h  = CH_horizontal(q_labels2d, nc, nd) # compute horizontal CH
    CH_pd = CH_pos_diagonal(q_labels2d, nc, nd) # compute positve diagonal CH
    CH_nd = CH_neg_diagonal(q_labels2d, nc, nd) # compute negative diagonal CH

    # step3: combine and return overall model CH
    return kmeans, np.sum((CH_v, CH_h, CH_pd, CH_nd), axis=0)
\end{lstlisting}

\begin{lstlisting}[language=Python, caption={Computing candidate CH}, captionpos=b, label={lst:chcand}]
def get_candidate_CH(kmeans, template, nc, nd):
    """computes overall CH along vertical and horizontal axes"""
    # step1: quantize rgb colourspace according to nc
    q_labels2d = RGB_quantize_candidate(kmeans, template, nc)
    
    # step2: compute vertical and horizontal CH's
    CH_v  = CH_vertical(q_labels2d, nc, nd) # compute vertical CH
    CH_h  = CH_horizontal(q_labels2d, nc, nd) # compute horizontal CH
    CH_pd = CH_pos_diagonal(q_labels2d, nc, nd) # compute positve diagonal CH
    CH_nd = CH_neg_diagonal(q_labels2d, nc, nd) # compute negative diagonal CH

    # step3: combine and return overall candidate CH
    return np.sum((CH_v, CH_h, CH_pd, CH_nd), axis=0)
\end{lstlisting}

\subsection{Computing the similarity measure}
The co-occurrence histogram intersection as expressed in
Equation~\ref{eqn:intersection} is computed by the code in
Listing~\ref{lst:intersection}

\begin{lstlisting}[language=Python, caption={Computing intersection of two CCH's}, captionpos=b, label={lst:intersection}]
def intersection(CH_1, CH_2, nc, nd):
    """computes the intersection between two Colour Co-occurrence Histograms""" 
    I = 0 # intersection
    for k in range(0, nd):
        for i in range(0, nc):
            for j in range(0, nc):
                I = I + np.minimum(CH_1[k,i,j], CH_2[k,i,j])
    return I # return the intersection of CH_1 and CH_2
\end{lstlisting}

\subsection{Co-occurrence histogram detection algorithm}
The detection algorithm as described in Figure~\ref{fig:design_algorithm_CH}
is computed by a coarse and fine grained search of the frame of interest. 
The implementation shown in Listings~\ref{lst:localisation} performs a two level
search. In the coarse search, the search window has has twice the dimensions of the
template we wish to locate and a step equal to the template dimensions. The
coarse search returns a sub-region of the frame which becomes the frame in the
fine grained search. 

The fine grained search then operates within the best location of the coarse
search with dimensions of the template and a step size of a quarter of the
template dimensions for accurate localisation of the template within the
sub-region identified within the coarse search.

\subsubsection{Localisation algorithm}
The localisation implementation is show in Listing~\ref{lst:localisation}.

\begin{lstlisting}[language=Python, caption={CCH detection localization algorithm}, captionpos=b, label={lst:localisation}]
def localize(template, frame, nc, nd, alpha=0.8):
    # obtain dimensions of frame, model and search window
    height, width = frame.shape[0], frame.shape[1] # frame dimensions
    model_height, model_width = template.shape[0], template.shape[1] # model dimensions
    sw_height0, sw_width0 = 2*model_height, 2*model_width # search window dimensions
    step_y = sw_height0//2 # y step for search window 0
    step_x = sw_width0//2 # x step for search window 0
    
    # obtain model CH
    kmeans, CH_m = CH_overall_model(template, nc, nd) 

    # coarse search
    y0, x0 = search(kmeans, frame, CH_m, sw_height0, sw_width0, step_y, step_x, nc, nd, alpha) 
    
    # fine search
    refined_frame = frame[y0:y0+sw_height0, x0:x0+sw_width0] # search within best coarse result
    sw_height1, sw_width1 = model_height, *model_width # search window dimensions
    step_y = sw_height1//4
    step_x = sw_width1//4
    y1, x1 = search(kmeans, refined_frame, CH_m, sw_height1, sw_width1, step_y, step_x, nc, nd, alpha)  
    return (x1, y1, model_width, model_height) # location of object in frame
\end{lstlisting}

\subsubsection{Search algorithm}
The search algorithm as defined in Figure~\ref{fig:design_algorithm_CH} is shown
in Listing~\ref{lst:search}
\begin{lstlisting}[language=Python, caption={CCH detection search algorithm}, captionpos=b, label={lst:search}]
def search(kmeans, frame, CH_m, sw_height, sw_width, step_y, step_x, nc, nd, alpha):
    """performs course search for particular model CH in frame"""
    height, width = frame.shape[0], frame.shape[1] # frame dimensions 
    I_mm = CH_intersection(CH_m, CH_m, nc, nd) # compute I_mm 
    I_best, y_best, x_best = 0, 0, 0
    
    for y in range(0,height-step_y-1,step_y):
        for x in range(0,width-step_x-1,step_x):
            candidate = frame[y:y+sw_height, x:x+sw_width] # candidate window 
            CH_c = CH_overall_candidate(kmeans, candidate, nc, nd) # compute candidate CH
            I_cm = CH_intersection(CH_c, CH_m, nc, nd)
            if(I_cm > alpha*I_mm and I_cm > I_best): # check I_cm match against threshold and best match
                I_best = I_cm # update best Intersection
                y_best, x_best = y, x # update best coordinates            
    return y_best, x_best # return best match
\end{lstlisting}

\section{Mean shift tracker}\label{implementation_mean_shift_tracker}
The implementation of the mean shift tracker follows directly from the algorithm
description in Section~\ref{theoretical_framework_mean_shift_algorithm}.

This Section begins by laying out some preliminaries choices for the algorithm
implementation, then details the implementation of the various functions and
their overall integration into a working mean shift tracker. The  

\subsection{Algorithm parameters}
The mean shift tracker has three parameters that can affect it's performance.
The maximum allowable mean shift step size (in pixels), $\epsilon$. The number
of bins used in computing our target and candidate PDFs $\hat{q}$ and
$\hat{p}$, and finally the size of the kernel initialised around the target
object.
An implementation should allow for variations of these parameters.

\subsection{Computing target and candidate PDFs}
Computing a PDF for a particular region of interest according to 
Equation~\ref{eqn:mean_shift_histogram} is performed by the \textit{get\_pdf
(template, m)} method shown in Listing~\ref{lst:pdf}.

\begin{lstlisting}[language=Python, caption={Function computing the PDF}, captionpos=b, label={lst:pdf}]
def get_pdf(template,m=8):
    """compute 2d colour histogram given template,bin number,m and kernel function"""
    # step1: limit pixels to elliptical region inscribed in rectangle
    height, width = template.shape[0], template.shape[1]
    y0, x0 = (height//2, width//2) # get center of kernel (treated as 0,0) == hy and hx 
    elliptical_mask = get_elliptical_mask(height, width)
       
    # step2: fetch pixel values
    hist = np.zeros(shape=(m, m)) # 2d-histogram to hold values 
    kernel, C = get_epanechnikov_kernel(template) # kernel and normalisation constant, C
    for y in range(0,height):
        for x in range(0,width): 
            if(elliptical_mask[y,x]==True): # only deal with points in the mask
                u, v = int((template[y,x,0]/256)*m),
                int((template[y,x,1]/256)*m) # get quantized R, G
                index_r, index_g = u//m, v//m # used to index 2d-histogram
                hist[index_r,index_g] = hist[index_r,index_g] + kernel[y,x] # add weighted point
    
    C = np.sum(hist) # normalisation constant
    histogram = np.true_divide(hist, C) # normalise histogram
    return histogram 
\end{lstlisting}

This above method is dependent on the implementation of the following support
methods. See Appendix~\ref{supportMS} for code listings of their implementation.
\begin{itemize}
    \item get\_elliptical\_mask (height, width)
    \item get\_euclidean\_distance (x1, x2):
    \item get\_epanechnikov\_weight (x):
    \item get\_epanechnikov\_kernel (roi):
    \item get\_pdf (roi, m):
\end{itemize}

\subsection{Computing the Bhattacharyyaa coefficient}
Computing the Bhattacharyyaa coefficient between two PDFs is implemented in
Listing~\ref{lst:BC} according to Equation~\ref{eqn:bhattacharyya}. 

\begin{lstlisting}[language=Python, caption={Function computing the Bhattacharyya coefficient}, captionpos=b, label={lst:BC}]
def bhatt_coeff(q, p):
    """compute the bhattacharyya coefficient between two 2D PDFs """
    height, width = q.shape[0], q.shape[1]
    batt = 0
    for y in range(0,height):
        for x in range(0,width):
            batt = batt + np.sqrt(q[y,x]*p[y,x])
    return batt
\end{lstlisting}

\subsection{Computing the pixel weights}
Computing the pixel weights within the region of interest is performed by the
method \textit{get\_weights (roi, q, p, m=8)} in Listing~\ref{lst:weights} according to Equation~\ref{eqn:mean_shift_weights}.

\begin{lstlisting}[language=Python, caption={Function computing kernel Weights}, captionpos=b, label={lst:weights}]
def pixel_weights(template, q, p, m=8):
    """compute the weights necessary for the mean shift algorithm given q and p"""
    # step1: limit pixels to elliptical region inscribed in rectangle
    height, width = template.shape[0],template.shape[1]
    y0, x0 = (height//2, width//2) # get centre 
    elliptical_mask = get_elliptical_mask(height, width)
    
    # step2: compute weights
    weights = np.zeros(shape=(height,width))
    for y in range(0,height):
        for x in range(0,width): 
            if(elliptical_mask[y,x]==True):  
                u, v = template[y,x,0], template[y,x,1] # get colour index, u 
                index_r, index_g = u//m, v//m # used to index 2d-histogram
                if(p[index_r,index_g]>0): # stop infinity division
                    weights[y,x] = np.sqrt(q[index_r,index_g]/p[index_r,index_g]) # comp weight
    return weights 
\end{lstlisting}

The implementation of Listing~\ref{lst:weights} requires the following methods to
be available. See Appendix~\ref{appendix_A} for code listings of their implementation. 
\begin{itemize}
    \item \textit{get\_elliptical\_mask (height, width)}
\end{itemize}

\subsection{Computing the mean shift vector}
Computing of the Mean Shift Vector is implemented in Listing~\ref{lst:msv}
according to %Equation~\ref{eqn:mean_shift_vector}.

\begin{lstlisting}[language=Python, caption={Function computing the mean shift vector}, captionpos=b, label={lst:msv}]
def mean_shift(template, weights):
    """compute the mean shift vector of the template based on the weights"""
    # step1: limit pixels to elliptical region inscribed in rectangle
    height, width = template.shape[0], template.shape[1]
    y0, x0 = (height//2, width//2) # get centre 
    elliptical_mask = get_elliptical_mask(height,width)

    # step2: estimate shift vector
    v_y,v_x = 0,0 # to hold estimated shift vector
    for y in range(0,height):
        for x in range(0,width): 
            if(elliptical_mask[y,x]==True):
                v_y = v_y + weights[y,x]*((y-y0)) 
                v_x = v_x + weights[y,x]*((x-x0))
    w_sum = np.sum(weights) # compute denominator of expression
    if(w_sum!=0):
        return int(v_y/w_sum),int(v_x/w_sum) # return (vy, vx)
    else:
        return 0,0 # return no shift
\end{lstlisting}

The implementation of Listing~\ref{lst:vector} requires the following methods to
be available. See Appendix~\ref{appendix_A} for code listings of their implementation. 
\begin{itemize}
    \item get\_elliptical\_mask (roi)
\end{itemize}

\subsection{Performing the mean shift loop}
The Mean Shift Loop is outlined by the Algorithm in
Figure~\ref{fig:design_mean_shift_tracking_algorithm}, the mean shift loop is
applied to a frame $\mathbf{f}_{k+1}$ given the model pdf of the object of
interest $\hat{q}$ and its location $\mathbf{c_0}$ in the previous frame
$\mathbf{f}_{k}$, to compute it's current position $\mathbf{c_1}$ in
$\mathbf{f}_{k+1}$

\begin{lstlisting}[language=Python, caption={Mean Shift Loop}, captionpos=b, label={lst:loop}]
def mean_shift_loop(frame, q, y0, x0, h, w, eps=5, m=8):
    """perform mean shift to get new location"""
    t1 = frame[y0:y0+h,x0:x0+w] # candidate template
    p0 = histogram(t1, m) # candidate histogram

    for i in range(0,10): # mean shift loop 20 iterations
        weights = get_pixel_weights(t1, q, p0, m) # calculate weights
        vy, vx = mean_shift(t1, weights) # calculate mean shift vector
        y1, x1 = int(y0+vy), int(x0+vx) # y1

        step_size = euclidean_distance((vy,vx),(0,0))
        if(step_size<eps): # likely to converge so check last
            return y1, x1 # found target object
            
        else: # step not small enough use battacharyya coefficient to refine step
            tc = frame[y1:y1+h,x1:x1+w] # get ROI at x1
            p1 = histogram(tc,m) # compute p(x1)
            batt0 = bhatt_coeff(q,p0) # similarity measure between q(x0) and p(x0) 
            batt1 = bhatt_coeff(q,p1) # similiarty measure betweem q(x0) and p(x1)

            while(batt0>batt1): # need to be better than origin rho
                vy, vx = vy//2, vx//2 # halve the step size
                y1, x1 = int(y0+vy), int(x0+vx) # get new smaller distance 
                tc = frame[y1:y1+h,x1:x1+w]
                p1 = histogram(tc, m)
                batt1 = bhatt_coeff(q, p1)
                if(np.abs(vy)<=1 and np.abs(vx)<=1): # avoid infinite loop
                    break
            return y1, x1
    return y1, x1
\end{lstlisting}

The implementation of the mean shift loop in Listing~\ref{lst:loop} relies on
the following functions being available. The implementations of these functions
are detailed in Appendix~\ref{appendix_B}.
\begin{itemize}
    \item get\_pdf (roi, m)
    \item get\_weights (roi, q, p)
    \item get\_euclidean\_distance (x1, x2)
    \item get\_msv (roi, pixel\_weights)
    \item get\_BC (q, p)
\end{itemize}

\section{Graphical user interface}
This Section deals with implementation of the GUI which interfaces with the
back-end classes to satisfy the system user requirements.  The GUI was
implemented by use of the pyQt5 python3 library and it's layout follows the
closely that of the mock-up design in Figure~\ref{fig:design_gui_mockup}

\subsection{GUI programming in the Qt framework}
Every implementation of a GUI application requires a QApplication object. This
object provides access to global variables such as the application directory and
screen size. It also provides an event loop. In this event loop, the application
processes in response to signals generated by GUI events such as button
presses or other events.
To handle the generation and response to events the pyQt framework provides the
signals and slots mechanism. An event generates a signal, and this signal is
connected to a slot which can be any callable function block describing what
action the model should take~\cite{Summerfield}.

\subsection{Layout}
The GUI window layout is done hierarchically in a simple layout. We have a top
level central view that takes on a two element horizontal layout, The left half
holds the view for the displaying the frame, and the right half is further split
vertically with the top half holding the view for the model of interest and the
bottom half containing the navigation buttons.
The results of the implementation can be seen in
Figure~\ref{fig:results/gui/first_look} of the Results Chapter.

\subsection{Controllers}
The MT systems' controllers comprise options within the file menu, toolbar,
navigation buttons, that a user can use to interact with the system. Each of
these controllers emit signals that are connected to a callable function within
the application (slot), specifying the action to be taken.
An example of this implementation mechanism is in the File I/O sequence
selection functionality. 
The sequenceSelectAction is a controller that can be added to the GUI ``File'' menu
bar and tool bar. Its implementation is detailed in Listing~\ref{lst:select}. 

\begin{lstlisting}[language=Python, caption={Code listing detailing sequence selection controller and connection to slot}, captionpos=b, label={lst:select}]
# snippet 1: create controller file select 
sequenceSelectAction = QAction(fileIcon,'Select Sequence', self) # Exit Action Object
sequenceSelectAction.setShortcut('Ctrl+O') # bind shortcut "Ctrl+O" open file
sequenceSelectAction.setStatusTip('Open Sequence') # status bar exit message
sequenceSelectAction.triggered.connect(self.openSequence) # connect QtGui quit() method 

# snippet 2: add sequence select action to file menu and tool bar
fileMenu.addAction(sequenceSelectAction) # Add sequenceSelectAction to fileMenu 
toolBar.addAction(sequenceSaveAction) # Add Seqeuence Save Object to Tool Bar
    
# snippet 3: callable slot for file select
@pyqtSlot()
def openSequence(self):
    """function to point to a folder with a desired sequence"""
    self.seq_dir = QFileDialog.getExistingDirectory(self,'Open File') # store path to sequence
    self.readSequence(self.seq_dir) # read in file names in sequence into self.paths

def readSequence(self, seq_dir):
    """read in sequence"""
    for path in sorted(os.listdir(seq_dir)):
        full_path = os.path.join(seq_dir, path)
        if os.path.isfile(full_path):
            self.seq_paths.append(full_path)
    
    self.seq_length = len(self.seq_paths) # update sequence length
    self.cur_index = 0 # reset image index
    self.cur_path = self.seq_paths[self.cur_index] # set cur image to first image in self.seq_paths
    self.loadImage() # load in first image
\end{lstlisting}

Listing~\ref{lst:select} details three snippets that define the creation of a
sequence select controller (snippet 1). It then shows how the this controller
added to the GUI menu bar and tool bar
defined, including it's connection to it's slot (snippet 2). The slot
\textit{openSequence ()} slot 
specifies the GUI response a user selection of the ``Select Sequence'' menu
option. It uses a QFileDialog to allow a user visual selection of a image
sequence folder in a specific directory. A call to the \textit{readSequence ()}
function then reads in a list of all the full paths to the images on the
specified directory, and displays the first image in the sequence to a user.

Other controllers in the GUI follow a similar implementation pattern, starting
with definition, addition to the GUI and connection to a callable slot.

\subsection{Views}
The GUI application has two views, a frame view to display the current image in the
sequence, and a smaller tracking view to display the detected or tracked object
within a particular frame.  
The tracking view is simply a QLabel which is a widget we can display images to.
The frame view requires extended functionality allows for user selection of
regions of interest as the initial detection step of the implemented detection
and tracking algorithms, thus making it a view and controller.

\subsubsection{User object detection functionality}
Listing~\ref{lst:image_view} details the implementation of this
functionality. The imageView class that subclasses the QLabel class is created.
This allows for overriding of the mouse click events to begin drawing a
rectangle upon a mouse click event, and stop drawing with a mouse release event.
This behaviour is facilitated by the QRubberband class. We emit the results as
a signal containing the origin of the top left corner $(x,y)$ of the rectangle, and it's
the width and height, $(w, h)$. This is the initial detection step of our
implemented kernel-based trackers.

\begin{lstlisting}[language=Python, caption={Code for User selection of regions/objects of interest}, captionpos=b, label={lst:image_view}]
class imageView(QLabel):
    """image view subclasses QLabel to render and add mouse events"""
    currentQRect = 0
    triggered = pyqtSignal() # to signal sent on mouse release
    def __init__(self):
        super(imageView, self).__init__()
        self.pixmap = QPixmap() # to store image
        self.cropLabel = QLabel() # user selected crop

    # Mouse Events
    def mousePressEvent(self, eventQMouseEvent):
        self.originQPoint = eventQMouseEvent.pos() # obtain origin of mouse click
        self.currentQRubberBand = QRubberBand(QRubberBand.Rectangle, self) # rubberband selection
        self.currentQRubberBand.setGeometry(QRect(self.originQPoint, QSize())) # 
        self.currentQRubberBand.show()

    def mouseMoveEvent(self, eventQMouseEvent):
        self.currentQRubberBand.setGeometry(QRect(self.originQPoint, eventQMouseEvent.pos()).normalized())

    def mouseReleaseEvent(self, eventQMouseEvent):
        QRect = self.currentQRubberBand.geometry() 
        self.currentQRect = (QRect.y(), QRect.x(), QRect.height(),
        QRect.width()) # Qrect refines coordinates of selection in original image
        self.currentQRubberBand.deleteLater() 
        cropPixmap = self.pixmap.copy(QRect) # copy only selection from current frame
        self.cropLabel.setPixmap(cropPixmap) # set the pixmap of tracking view
        self.triggered.emit() # emit signal that selection complete
\end{lstlisting}


\section{Integration}
This Section highlights the points of integration between the front- and
back-end implementations.
