\chapter{Implementation}\label{chapter_implementation}
This Chapter details the implementations of the relevant software modules of the
overall Motion Tracking (MT) System. They constitute the practical work performed in line with the system design
developed in Chapter~\ref{chapter_design} and are firmly grounded within the theoretical framework
established in Chapter~\ref{chapter_theoretical_framework}.

The focus of this Chapter in terms of the back-end is in the presentation of the
procedural algorithmic implementation, independent of class structure. The
reason behind this is so that the parallels to the underlying theory in
Chapter~\ref{chapter_theoretical_framework} can better be appreciated.
The only differences between the class-based implementations according to the
UML Diagram in Figure~\ref{fig:design_uml} and those presented here, is that the
class functions are not procedurally implemented. The procedural implementation
was presented here is also meant enhance clarity and reproducibility. The class based
implementations can be seen in Appendix~\ref{appendix_implementation}.

This Chapter also presents the various implementation details of the front-end
GUI implementation. Attention is drawn to the specific functionality to
facilitate various methods of user interaction with the MT System. GUI
components with similar functionality or implementation patterns are presented
once. Appendix~\ref{appendix_gui} can be queried for the complete
implementation. 

This Chapter concludes with a presentation of the implementation details of the
integration of the back-end tracker functionality with the front-end GUI in the
overall MT System. The integrated project can be found at the project GitHub
repository~\cite{repository}~\footnote{https://github.com/Claude47/thesis product}


\section{Template Matching Tracker}\label{implementation_tm}
This Section details the implementation of the two variants of Template matching
tracker, namely the simple template tracker (STT) and the adaptive template
tracker (ATT) as described in Section~\ref{theoretical_framework_tm}. The template trackers were
implemented using the highly optimized \lstinline{cv2.matchTemplate ()} template
matching function provided by the \textbf{OpenCV} library.  

The reason for reliance on the \textbf{OpenCV} implementation is based on the fact that
preliminary experimentation and research revealed that the execution time of
naive implementation convolution was orders of magnitude from what would be
required to feasibly implement a template tracker. 

The necessary optimisations are achievable by conversion to the frequency domain using a
Short-Time Fourier Transform (STFT). The requisite and time into an optimised
correlation implementation for a template matching tracker was deemed tangential
and beyond the scope of this study. However, a comprehensive treatment of this
optimisation is provided by~\cite{Usevitch2010}. The two STT and ATT algorithm
implementations for detecting and object in $\mathbf{f}_k$ are presented below.

\subsection{Simple template tracker}
The implementation of the STT follows directly from the simple algorithm
proposed in Figure~\ref{fig:design_template_tracking_algorithm}.
Listing~\ref{lst:STT} details the algorithm for the STT\@. Given a frame,
$\mathbf{f}_k$ and a
template $\mathbf{T}$ of the target object to be located, the \textbf{OpenCV} \lstinline{cv.matchTemplate ()} function
returns the \lstinline{res} array. This array has entries of the similarity
measure (normed sum of square differences) and the
corresponding coordinates for each of the overlaps in the 2D convolution of the
template and frame. The entries range from $\left[ 0,1 \right]$ with 0
corresponding to the worst possible match, and 1 corresponding to the best
possible match. The \lstinline{threshold} parameter defines error margin of
similarity. 
The function returns the location of the best match provided it exceeds the
specified threshold.

\begin{lstlisting}[language=Python, caption={STT tracking loop}, captionpos=b, label={lst:STT}]
def simpleTT(frame, template, threshold=0.8):
    """perform simple template matching"""
    res = cv.matchTemplate(frame, template, cv.TM_CCOEFF_NORMED)
    if(np.max(res)<threshold): # control threshold
        return -1
    loc = np.where(res==np.max(res))
    loc = (loc[0].flatten()[0], loc[1].flatten()[0])        
    return loc
\end{lstlisting}

\subsection{Adaptive template tracker}
As detailed in the ATT step in the algorithm of Figure~\ref{fig:design_template_tracking_algorithm}.
The difference between the ATT and STT implementations is that the ATT has the
aditional template update step, as seen by the return value.

\begin{lstlisting}[language=Python, caption={ATT tracking loop}, captionpos=b, label={lst:ATT}]
def adaptiveTT(frame, template, threshold):
    """perform adaptive template matching"""
    res = cv.matchTemplate(frame, template, cv.TM_CCOEFF_NORMED)
    if(np.max(res)<threshold): # control threshold
        return -1
    loc = np.where(res==np.max(res))
    loc = (loc[0].flatten()[0], loc[1].flatten()[0])        
    
    h, w = template.shape[:2] # get template dimensions
    template = frame[loc[0]:loc[0]+h, loc[1]: loc[1]+w] # update template
    return loc, template
\end{lstlisting}

\subsection{Template Matching Tracker}
The two implementations in Listings~\ref{lst:STT} and~\ref{lst:ATT} are simply
detectors. The tracking is performed by integration into a loop and application
of the chosen template detector to each frame within the selected sequence.
A notable implementation detail is that the ssd measure is computed on greyscale
images.

\section{Colour Co-occurrence Histogram Detector}\label{implementation_ch}
The implementation of the CH-Detector follows directly from the theoretical
development presented in Section~\ref{theoretical_framework_ch}.

\subsection{Computing model and candidate CCHs}
The model and candidate CCHs are computed by the \lstinline{get_model_CH (template,nc,nd)} 
and \lstinline{get_candidate_CH (kmeans,template,nc,nd)}
functions respectively.

The model's euclidean RGB colour-space is quantized by use of the kmeans
clustering algorithm and the colour co-occurrence histogram computed using these
quantization levels. The kmeans centroids are returned and subsequently used to
quantize candidates when computing their CCH are shown in
Listings~\ref{lst:chmod} and~\ref{lst:chcand}.

\begin{lstlisting}[language=Python, caption={Computing model CH}, captionpos=b, label={lst:chmod}]
def get_model_CH(template, nc, nd):
    """computes overall CH along vertical and horizontal axes"""
    # step1: quantize rgb colourspace according to nc
    kmeans, q_labels2d = RGB_quantize_model(template, nc)
    
    # step2: compute vertical and horizontal CH's
    CH_v  = CH_vertical(q_labels2d, nc, nd) # compute vertical CH
    CH_h  = CH_horizontal(q_labels2d, nc, nd) # compute horizontal CH
    CH_pd = CH_pos_diagonal(q_labels2d, nc, nd) # compute positve diagonal CH
    CH_nd = CH_neg_diagonal(q_labels2d, nc, nd) # compute negative diagonal CH

    # step3: combine and return overall model CH
    return kmeans, np.sum((CH_v, CH_h, CH_pd, CH_nd), axis=0)
\end{lstlisting}

\begin{lstlisting}[language=Python, caption={Computing candidate CH}, captionpos=b, label={lst:chcand}]
def get_candidate_CH(kmeans, template, nc, nd):
    """computes overall CH along vertical and horizontal axes"""
    # step1: quantize rgb colourspace according to nc
    q_labels2d = RGB_quantize_candidate(kmeans, template, nc)
    
    # step2: compute vertical and horizontal CH's
    CH_v  = CH_vertical(q_labels2d, nc, nd) # compute vertical CH
    CH_h  = CH_horizontal(q_labels2d, nc, nd) # compute horizontal CH
    CH_pd = CH_pos_diagonal(q_labels2d, nc, nd) # compute positve diagonal CH
    CH_nd = CH_neg_diagonal(q_labels2d, nc, nd) # compute negative diagonal CH

    # step3: combine and return overall candidate CH
    return np.sum((CH_v, CH_h, CH_pd, CH_nd), axis=0)
\end{lstlisting}

\subsection{Computing the similarity measure}
The CCH intersection as expressed in
Equation~\ref{eqn:intersection} is computed by the code in
Listing~\ref{lst:intersection}. It is subsequently used localise the model
objects within a particular $\mathbf{f}_k$.

\begin{lstlisting}[language=Python, caption={Computing intersection of two CCH's}, captionpos=b, label={lst:intersection}]
def intersection(CH_1, CH_2, nc, nd):
    """computes the intersection between two Colour Co-occurrence Histograms""" 
    I = 0 # intersection
    for k in range(0, nd):
        for i in range(0, nc):
            for j in range(0, nc):
                I = I + np.minimum(CH_1[k,i,j], CH_2[k,i,j])
    return I # return the intersection of CH_1 and CH_2
\end{lstlisting}

\subsection{Co-occurrence histogram detection algorithm}
The detection algorithm as described in Figure~\ref{fig:design_algorithm_CH}
is computed by a coarse and fine grained search of the frame of interest $\mathbf{f}_k$. 
The implementation shown in Listing~\ref{lst:localisation} performs a two level
search. In the coarse search, the search window has has twice the dimensions of the
template, $\mathbf{T}$ we wish to locate and a step equal to the $\mathbf{T}$'s dimensions. The
coarse search returns a sub-region of $\mathbf{f}_k$ which becomes the
frame ${\mathbf{f}_k}^*$ in the fine grained search. 

The fine grained search then operates within ${\mathbf{f}_k}^*$
search with dimensions of $\mathbf{T}$ and a step size of a quarter of the
template dimensions for accurate localisation of $\mathbf{T}$ within
${\mathbf{f}_k}^*$ (which was the localization region of the coarse search).

\subsubsection{Localisation algorithm}
The implementation of the localisation algorithm is show in Listing~\ref{lst:localisation}. This
function orchestrasted the different search levels in the 
\begin{lstlisting}[language=Python, caption={CCH detection localization algorithm}, captionpos=b, label={lst:localisation}]
def localize(template, frame, nc, nd, alpha=0.8):
    # obtain dimensions of frame, model and search window
    height, width = frame.shape[0], frame.shape[1] # frame dimensions
    model_height, model_width = template.shape[0], template.shape[1] # model dimensions
    sw_height0, sw_width0 = 2*model_height, 2*model_width # search window dimensions
    step_y = sw_height0//2 # y step for search window 0
    step_x = sw_width0//2 # x step for search window 0
    
    # obtain model CH
    kmeans, CH_m = CH_overall_model(template, nc, nd) 

    # coarse search
    y0, x0 = search(kmeans, frame, CH_m, sw_height0, sw_width0, step_y, step_x, nc, nd, alpha) 
    
    # fine search
    refined_frame = frame[y0:y0+sw_height0, x0:x0+sw_width0] # search within best coarse result
    sw_height1, sw_width1 = model_height, *model_width # search window dimensions
    step_y = sw_height1//4
    step_x = sw_width1//4
    y1, x1 = search(kmeans, refined_frame, CH_m, sw_height1, sw_width1, step_y, step_x, nc, nd, alpha)  
    return (x1, y1, model_width, model_height) # location of object in frame
\end{lstlisting}

\subsubsection{Search algorithm}
The search algorithm as defined in Figure~\ref{fig:design_algorithm_CH} is shown
in Listing~\ref{lst:search}
\begin{lstlisting}[language=Python, caption={CCH detection search algorithm}, captionpos=b, label={lst:search}]
def search(kmeans, frame, CH_m, sw_height, sw_width, step_y, step_x, nc, nd, alpha):
    """performs course search for particular model CH in frame"""
    height, width = frame.shape[0], frame.shape[1] # frame dimensions 
    I_mm = CH_intersection(CH_m, CH_m, nc, nd) # compute I_mm 
    I_best, y_best, x_best = 0, 0, 0
    
    for y in range(0,height-step_y-1,step_y):
        for x in range(0,width-step_x-1,step_x):
            candidate = frame[y:y+sw_height, x:x+sw_width] # candidate window 
            CH_c = CH_overall_candidate(kmeans, candidate, nc, nd) # compute candidate CH
            I_cm = CH_intersection(CH_c, CH_m, nc, nd)
            if(I_cm > alpha*I_mm and I_cm > I_best): # check I_cm match against threshold and best match
                I_best = I_cm # update best Intersection
                y_best, x_best = y, x # update best coordinates            
    return y_best, x_best # return best match
\end{lstlisting}

\section{Mean Shift Tracker}\label{implementation_mean_shift_tracker}
The implementation of the mean shift tracker (MST) follows directly from the algorithm
description in Section~\ref{theoretical_framework_mean_shift_algorithm}.

This Section begins by laying out some preliminaries choices for the algorithm
implementation, then details the implementation of the various functions and
their overall integration into a working MST\@. 

\subsection{Algorithm parameters}
The MST has three parameters that can affect it's performance.
The maximum allowable mean shift step size (in pixels), $\epsilon$. The number
of bins $m$ used in computing our target and candidate PDFs ($\hat{q}$  and
$\hat{p}$), and finally the user defined size of the kernel initialised around the target
object in $\mathbf{f}_k$ of the tracking sequence.
An MST implementation should allow for variations of these parameters.

\subsection{Computing target and candidate PDFs}
Computing a PDF for a particular region of interest according to 
Equation~\ref{eqn:mean_shift_histogram} is performed by the \lstinline{get_pdf (template, m)} 
method shown in Listing~\ref{lst:pdf}.

One computational optimisation not mentioned in
Section~\ref{theoretical_framework_mean_shift_algorithm} that not all RGB
channels need to be used. As suggested by~\cite{Valenti2003}, we can select two
RGB channels with a low correlation, cutting out a third of requisite the computation. 

\begin{lstlisting}[language=Python, caption={Function computing the PDF}, captionpos=b, label={lst:pdf}]
def get_pdf(template,m=16):
    """compute 2d colour histogram given template,bin number,m and kernel function"""
    # step1: limit pixels to elliptical region inscribed in rectangle
    height, width = template.shape[0], template.shape[1]
    y0, x0 = (height//2, width//2) # get center of kernel (treated as 0,0) == hy and hx 
    elliptical_mask = get_elliptical_mask(height, width)
       
    # step2: fetch pixel values
    hist = np.zeros(shape=(m, m)) # 2d-histogram to hold values 
    kernel, C = get_epanechnikov_kernel(template) # kernel and normalisation constant, C
    for y in range(0,height):
        for x in range(0,width): 
            if(elliptical_mask[y,x]==True): # only deal with points in the mask
                u, v = template[y,x,0], template[y,x,1] # get quantized R, G
                index_r, index_g = u//(256//m), v//(256//m) # used to index 2d-histogram
                hist[index_r,index_g] = hist[index_r,index_g] + kernel[y,x] # add weighted point
    
    C = np.sum(hist) # normalisation constant
    histogram = np.true_divide(hist, C) # normalise histogram
    return histogram 
\end{lstlisting}

This above method is dependent on the implementation of the following support
methods. See Appendix~\ref{appendix_supportMS} for code listings of their implementation.
\begin{itemize}
    \item \lstinline{get_elliptical_mask (height, width)}
    \item \lstinline{get_euclidean_distance (x1, x2)}
    \item \lstinline{get_epanechnikov_weight (x)}
    \item \lstinline{get_epanechnikov_kernel (roi)}
    \item \lstinline{get_pdf (roi, m)}
\end{itemize}

\subsection{Computing the Bhattacharyyaa coefficient}
Computing the Bhattacharyyaa coefficient between two PDFs is implemented in
Listing~\ref{lst:BC} according to Equation~\ref{eqn:bhattacharyya}. 

\begin{lstlisting}[language=Python, caption={Function computing the Bhattacharyya coefficient}, captionpos=b, label={lst:BC}]
def bhatt_coeff(q, p):
    """compute the bhattacharyya coefficient between two 2D PDFs """
    height, width = q.shape[0], q.shape[1]
    batt = 0
    for y in range(0,height):
        for x in range(0,width):
            batt = batt + np.sqrt(q[y,x]*p[y,x])
    return batt
\end{lstlisting}

\subsection{Computing the pixel weights}
Computing the pixel weights within the region of interest is performed by the
method \lstinline{get_weights (roi, q, p, m)} in Listing~\ref{lst:weights} according to Equation~\ref{eqn:mean_shift_weights}.

\begin{lstlisting}[language=Python, caption={Function computing kernel Weights}, captionpos=b, label={lst:weights}]
def get_weights(template, q, p, m=16):
    """compute the weights necessary for the mean shift algorithm given q and p"""
    # step1: limit pixels to elliptical region inscribed in rectangle
    height, width = template.shape[0],template.shape[1]
    y0, x0 = (height//2, width//2) # get centre 
    elliptical_mask = get_elliptical_mask(height, width)
    
    # step2: compute weights
    weights = np.zeros(shape=(height,width))
    for y in range(0,height):
        for x in range(0,width): 
            if(elliptical_mask[y,x]==True):  
                u, v = template[y,x,0], template[y,x,1] # get colour index, u 
                index_r, index_g = u//m, v//m # used to index 2d-histogram
                if(p[index_r,index_g]>0): # stop infinity division
                    weights[y,x] = np.sqrt(q[index_r,index_g]/p[index_r,index_g]) # comp weight
    return weights 
\end{lstlisting}

The implementation of Listing~\ref{lst:weights} requires the following methods to
be available. See Appendix~\ref{appendix_supportMS} for code listings of their implementation. 
\begin{itemize}
    \item \lstinline{get_elliptical_mask (height, width)}
\end{itemize}

\subsection{Computing the mean shift vector}
Computing of the mean shift vector, $\vec{v}$ is implemented in Listing~\ref{lst:msv}
according to Equation~\ref{eqn:mean_shift_vector}.

\begin{lstlisting}[language=Python, caption={Function computing the mean shift vector}, captionpos=b, label={lst:msv}]
def mean_shift(template, weights):
    """compute the mean shift vector of the template based on the weights"""
    # step1: limit pixels to elliptical region inscribed in rectangle
    height, width = template.shape[0], template.shape[1]
    y0, x0 = (height//2, width//2) # get centre 
    elliptical_mask = get_elliptical_mask(height,width)

    # step2: estimate shift vector
    v_y,v_x = 0,0 # to hold estimated shift vector
    for y in range(0,height):
        for x in range(0,width): 
            if(elliptical_mask[y,x]==True):
                v_y = v_y + weights[y,x]*((y-y0)) 
                v_x = v_x + weights[y,x]*((x-x0))
    w_sum = np.sum(weights) # compute denominator of expression
    if(w_sum!=0):
        return int(v_y/w_sum),int(v_x/w_sum) # return (vy, vx)
    else:
        return 0,0 # return no shift
\end{lstlisting}

The implementation of Listing~\ref{lst:msv} requires the following methods to
be available. See Appendix~\ref{appendix_supportMS} for code listings of their implementation. 
\begin{itemize}
    \item \lstinline{get_elliptical_mask (roi)}
\end{itemize}

\subsection{Performing the mean shift loop}
The mean shift loop is outlined by the algorithm in
Figure~\ref{fig:design_mean_shift_tracking_algorithm}, the mean shift loop is
applied to a frame $\mathbf{f}_{k+1}$ given the model pdf of the object of
interest $\hat{q}$ and its location $\mathbf{c_0}$ in the previous frame
$\mathbf{f}_{k}$, to compute it's current position $\mathbf{c_1}$ in
$\mathbf{f}_{k+1}$ according to Equation~\ref{eqn:mean_shift_procedure} 

\begin{lstlisting}[language=Python, caption={Mean shift loop}, captionpos=b, label={lst:loop}]
def mean_shift_loop(frame, q, y0, x0, h, w, eps=5, m=16):
    """perform mean shift to get new location"""
    t1 = frame[y0:y0+h,x0:x0+w] # candidate template
    p0 = histogram(t1, m) # candidate histogram

    for i in range(0,10): # mean shift loop 20 iterations
        weights = get_pixel_weights(t1, q, p0, m) # calculate weights
        vy, vx = mean_shift(t1, weights) # calculate mean shift vector
        y1, x1 = int(y0+vy), int(x0+vx) # y1

        step_size = euclidean_distance((vy,vx),(0,0))
        if(step_size<eps): # likely to converge so check last
            return y1, x1 # found target object
            
        else: # step not small enough use battacharyya coefficient to refine step
            tc = frame[y1:y1+h,x1:x1+w] # get ROI at x1
            p1 = histogram(tc,m) # compute p(x1)
            batt0 = bhatt_coeff(q,p0) # similarity measure between q(x0) and p(x0) 
            batt1 = bhatt_coeff(q,p1) # similiarty measure betweem q(x0) and p(x1)

            while(batt0>batt1): # need to be better than origin rho
                vy, vx = vy//2, vx//2 # halve the step size
                y1, x1 = int(y0+vy), int(x0+vx) # get new smaller distance 
                tc = frame[y1:y1+h,x1:x1+w]
                p1 = histogram(tc, m)
                batt1 = bhatt_coeff(q, p1)
                if(np.abs(vy)<=1 and np.abs(vx)<=1): # avoid infinite loop
                    break
            return y1, x1
    return y1, x1
\end{lstlisting}

The implementation of the mean shift loop in Listing~\ref{lst:loop} relies on
the following functions being available. The implementations of these functions
are detailed in Appendix~\ref{appendix_supportMS}.
\begin{itemize}
    \item get\_pdf (roi, m)
    \item get\_weights (roi, q, p)
    \item get\_euclidean\_distance (x1, x2)
    \item get\_msv (roi, pixel\_weights)
    \item get\_BC (q, p)
\end{itemize}

\section{Graphical User Interface}\label{implementation_gui}
This Section deals with implementation of the GUI which interfaces with the
back-end classes to satisfy the system user requirements.  The GUI was
implemented by use of the \textbf{PyQt5} and \textbf{Python3} library. It's
layout follows closely that of the mock-up design in
Figure~\ref{fig:design_gui_mockup}

\subsection{GUI programming in the Qt framework}
Every implementation of a GUI application requires a \lstinline{QApplication} object. This
object provides access to global variables such as the application directory and
screen size. It also provides an event loop. In this event loop, the application
processes in response to signals generated by GUI events such as button
presses or other events.
To handle the generation and response to events the \textbf{PyQt} framework provides the
signals and slots mechanism. An event generates a signal, and this signal is
connected to a slot which can be any callable function block describing what
action the model should take~\cite{Summerfield}.

\subsection{Layout}
The GUI window layout is done hierarchically in a simple layout. We have a top
level central view that takes on a two element horizontal layout, The left half
holds the view for the displaying the frame, and the right half is further split
vertically with the top half holding the view for the model of interest and the
bottom half containing the navigation buttons.
The results of the implementation can be seen in
Figure~\ref{fig:gui_integration} of the results Chapter.

\subsection{Controllers}
The MT systems' controllers comprise options within the file menu, toolbar,
navigation buttons, that a user can use to interact with the system. Each of
these controllers emit signals that are connected to a callable function within
the application (slot), specifying the action to be taken.
An example of this implementation mechanism is in the file I/O sequence
selection functionality. 
The \lstinline{sequenceSelectAction} is a controller that can be added to the GUI ``File'' menu
bar and tool bar. Its implementation is detailed in Listing~\ref{lst:select}. 

\begin{lstlisting}[language=Python, caption={Code listing detailing sequence selection controller and connection to slot}, captionpos=b, label={lst:select}]
# snippet 1: create controller file select 
sequenceSelectAction = QAction(fileIcon,'Select Sequence', self) # Exit Action Object
sequenceSelectAction.setShortcut('Ctrl+O') # bind shortcut "Ctrl+O" open file
sequenceSelectAction.setStatusTip('Open Sequence') # status bar exit message
sequenceSelectAction.triggered.connect(self.openSequence) # connect QtGui quit() method 

# snippet 2: add sequence select action to file menu and tool bar
fileMenu.addAction(sequenceSelectAction) # Add sequenceSelectAction to fileMenu 
toolBar.addAction(sequenceSaveAction) # Add Seqeuence Save Object to Tool Bar
    
# snippet 3: callable slot for file select
@pyqtSlot()
def openSequence(self):
    """function to point to a folder with a desired sequence"""
    self.seq_dir = QFileDialog.getExistingDirectory(self,'Open File') # store path to sequence
    self.readSequence(self.seq_dir) # read in file names in sequence into self.paths

def readSequence(self, seq_dir):
    """read in sequence"""
    for path in sorted(os.listdir(seq_dir)):
        full_path = os.path.join(seq_dir, path)
        if os.path.isfile(full_path):
            self.seq_paths.append(full_path)
    
    self.seq_length = len(self.seq_paths) # update sequence length
    self.cur_index = 0 # reset image index
    self.cur_path = self.seq_paths[self.cur_index] # set cur image to first image in self.seq_paths
    self.loadImage() # load in first image
\end{lstlisting}

Listing~\ref{lst:select} details three snippets that define the creation of a
sequence select controller (snippet 1). It then shows how the this controller
added to the GUI menu bar and tool bar
defined, including it's connection to it's slot (snippet 2). The slot
\lstinline{openSequence ()} slot 
specifies the GUI response a user selection of the ``Select Sequence'' menu
option. It uses a \lstinline{QFileDialog} to allow a user visual selection of a image
sequence folder in a specific directory. A call to the \lstinline{readSequence ()}
function then reads in a list of all the full paths to the images on the
specified directory, and displays the first image in the sequence to a user.

Other controllers in the GUI follow a similar implementation pattern, starting
with definition, addition to the GUI and connection to a callable slot.

\subsection{Views}
The GUI application has two views, a frame view to display the current image in the
sequence, and a smaller tracking view to display the detected or tracked object
within a particular frame.  
The tracking view is simply a \lstinline{QLabel} which is a widget we can display images to.
The frame view requires extended functionality allows for user selection of
regions of interest as the initial detection step of the implemented detection
and tracking algorithms, thus making it a view and controller.

\subsubsection{User object selection functionality}
To allow user selection of regions of interest, the \lstinline{imageView} class
that subclasses the \lstinline{QLabel} class is created.
Listing~\ref{lst:image_view} details the implementation of the user object
selection functionality.This allows for overriding of the mouse click events to
begin drawing a rectangle upon a mouse click event, and stop drawing with a
mouse release event.  This behaviour is facilitated by the
\lstinline{QRubberband} class. We emit the results as a signal containing the
origin of the top left corner $(x,y)$ of the rectangle, and it's the width and
height, $(w, h)$. This is the initial detection step of our implemented
kernel-based trackers.

\begin{lstlisting}[language=Python, caption={Code for User selection of regions/objects of interest}, captionpos=b, label={lst:image_view}]
class imageView(QLabel):
    """image view subclasses QLabel to render and add mouse events"""
    currentQRect = 0
    triggered = pyqtSignal() # to signal sent on mouse release
    def __init__(self):
        super(imageView, self).__init__()
        self.pixmap = QPixmap() # to store image
        self.cropLabel = QLabel() # user selected crop

    # Mouse Events
    def mousePressEvent(self, eventQMouseEvent):
        self.originQPoint = eventQMouseEvent.pos() # obtain origin of mouse click
        self.currentQRubberBand = QRubberBand(QRubberBand.Rectangle, self) # rubberband selection
        self.currentQRubberBand.setGeometry(QRect(self.originQPoint, QSize())) # 
        self.currentQRubberBand.show()

    def mouseMoveEvent(self, eventQMouseEvent):
        self.currentQRubberBand.setGeometry(QRect(self.originQPoint, eventQMouseEvent.pos()).normalized())

    def mouseReleaseEvent(self, eventQMouseEvent):
        QRect = self.currentQRubberBand.geometry() 
        self.currentQRect = (QRect.y(), QRect.x(), QRect.height(),
        QRect.width()) # Qrect refines coordinates of selection in original image
        self.currentQRubberBand.deleteLater() 
        cropPixmap = self.pixmap.copy(QRect) # copy only selection from current frame
        self.cropLabel.setPixmap(cropPixmap) # set the pixmap of tracking view
        self.triggered.emit() # emit signal that selection complete
\end{lstlisting}

\section{Integration}
This Section highlights the points of integration between the front- and
back-end implementations. It presents the integration of the MST tracker into
the front-end GUI\@. The integration of the other back-end trackers and detectors
follows a similar pattern.

The integration of front- and back-end subsystems should allow for effective
user interaction with the MT System in a variety of ways (user flows). 

The sequence diagram in Figure~\ref{fig:implementation_sequence_diagram} presents
one such user flow from perspective of a user trying to load and image sequence,
navigate it and initialise a mean shift tracker (MST) on an object in a particular
frame of interest. The sequence diagram details three communicating entities,
the User, the GUI and the MST tracker.

The user selects an image sequence of interest, navigating said
sequence and selecting the mean shift algorithm to be performed on the frame of
interest.
Following the algorithm choice, the user is queried for region of interest by
the GUI\@. Once the user selection has been made, the GUI initialises the MST
tracker with the template of the selected model in $\mathbf{f}_0$.

It then makes synchronous calls to the MST for coordinates of the object of
interest in $\mathbf{f}_{k+1},\ldots,\mathbf{f}_{k+n}$, these calls are returned
asynchronously by the MST according to the complexity of the particular loop
which varies.

\Figure[width=1\columnwidth]{Sequence diagram of sample user flow}{implementation_sequence_diagram} 



\begin{lstlisting}[language=Python, caption={Mean shift tracker integration in
GUI}, captionpos=b, label={lst:integration}]

# step1: import relevant back-end class and initialise it
from back_end.trackerMS import trackerMS # mean shift tracker
self.MST = trackerMS() # initialise MST object

# step2: controller signal to callable slot
# Mean Shift
meanShiftTrackingAction = QAction(trackIcon,'Mean Shift Tracker', self)
meanShiftTrackingAction.setShortcut('Ctrl+4')
meanShiftTrackingAction.setStatusTip('Apply Mean Shift Tracking Algorithm')
meanShiftTrackingAction.triggered.connect(self.meanShiftTracking) # connect to slot

# step3: mean shift tracker slot
@pyqtSlot()
def meanShiftTracking(self):
    self.statusBar().showMessage('Status: Tracking (Mean Shift)')
    QMessageBox.about(self, "Mean Shift Tracking", "Select Object to Track")
    self.imageLabel.triggered.connect(self.meanShiftSelection)

def meanShiftSelection(self):
    self.meanShiftTrackingInit()
    self.MST_timer = QTimer() # timer for frame rate
    self.MST_timer.timeout.connect(self.meanShiftTrackingLoop) # connect timeouts MSloop
    self.MST_timer.start(int(self.mst_delay * 1000)) # set timer countdown rate

def meanShiftTrackingInit(self):
    """initialise meanShiftTracking"""
    self.alg_running = True # tell system and Algorithm is running
    print(self.imageLabel.currentQRect)
    self.MST.setup(self.cur_img, self.imageLabel.currentQRect) # setup mean shift tracker with coords
    self.y0 = self.imageLabel.currentQRect[0]
    self.x0 = self.imageLabel.currentQRect[1]
    self.h = self.imageLabel.currentQRect[2]
    self.w = self.imageLabel.currentQRect[3]
    self.nextImage() # load frame 1

def meanShiftTrackingLoop(self):
    """implement loop"""   
    if(self.cur_index<self.seq_length-1):
        coords = self.MST.track(self.cur_img) # track    
        print("coords: ", coords)
        self.y0 = coords[0]
        self.x0 = coords[1]
        self.nextImage() # load image    
    else:
        self.MST_timer.stop() # terminate algorithm
        self.algorithm_reset() # cleanup and reset
\end{lstlisting}

The code in Listing~\ref{lst:integration} details three steps necessary in the
integration of the MST with the front-end GUI\@. The
\lstinline{meanShiftTracking (self)} slot is triggered by selection of the Mean
Shift Tracker from the Algorithm menu. This function set the
\lstinline{meanShiftSelection (self)} function to be triggered by the 
\lstinline{triggered} signal emitted at the end of mouse
release event in the customised \lstinline{imageView} class. 

The \lstinline{meanShiftSelection (self)} callback then initialised the tracker with
the selection at captured at the end of the user mouse click. Furthermore, it
sets up a timer that periodically triggers the
\lstinline{meanShiftTrackingLoop (self)}, which is called for each
$\mathbf{f}_k$ in the particular sequence, in which is updates the displayed
bounding box for each tracker response.

The integration of the other back-end algorithms into the GUI follow a similar
process.
This Chapter has presented the implementation details of the front- and back-end
subsystems of the MT System, as well as the steps taken for the integration of
the back-end functionality into the front-end.
Chapter~\ref{chapter_results} will present the performance of the individual
implementations and the overall integrated system.
