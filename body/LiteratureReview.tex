\chapter{Literature Review}\label{chapter_literature_review}
This Literature Review is structured as follows: It begins by establishing key
definitions necessary for navigating the literature.

The Review then broadly details the general approach to solving of Motion Tracking
problems. This is followed by a study of the progression of the field,
specifically reviewing prominent “classical” pre-deep learning approaches to the
problem.

\section{Key Terms}
The field of Computer vision is riddled with similar sounding terms whose intent
can get muddled during the discussion of concepts.  The purpose of this section
is to establish the intent of these keywords as they are used later in this text.

Detailed below are definitions of important terms within the field of computer
vision, some of which are used rather loosely within the literature or are
despite seeming interchangeable may refer to different concepts. 

\textbf{Image Segmentation}: This is the process of partitioning an image into multiple
sets of pixels with the goal of making the image easier to analyse, it typically
involves finding boundary lines of objects. 

\textbf{Motion Segmentation}: Motion Segmentation refers to the labelling of pixels that
are associated with each independently moving 3D object in sequence of images
that can feature multiple motions.~\cite{Tekalp2014}

\textbf{Object Recognition}: This a method by which Given an image, the objects within
said image are detected and classified as one of a set of predefined Object
Category i.e.\ ``What Object (s) are in this frame?''

\textbf{Object Detection}: Object Detection can be seen as targeted recognition, it deals
with finding instances of objects within a given image or video frame, this
technique usually makes use of extracted features and learning algorithms to
recognize instances of an object category.  I.e. “Where is this particular
object in this frame?”

\textbf{Object Classification}: This is a broader treatments than Detection, where a
system is not only able to identify objects within images but can differentiate
between and label different classes of objects within the image. 

\textbf{Optical Flow}: refers to the pattern of apparent motion of objects, surfaces and
edges in a visual scene caused by the relative motion between an observer and a
scene. Optic flow fields can be used to structure or segment a scene~\cite{Forsyth2012}.

\textbf{Optical Flow Segmentation}: Refers to the grouping together of optical flow
vectors that are associated with the same 3-D motion or structure. This problem
is identical to motion segmentation provided a dense optical flow field~\cite{Tekalp2014}.

\textbf{Object Tracking}: Object tracking also known as Video Tracking refers to the use
of sensor measurements to determine the location, path and characteristics of
objects of interest~\cite{Challa2011}.
 
\textbf{Motion Estimation}: A fundamental problem in video processing that is concerned
with determining motion vectors that describe the transformation of one 2D
image to another over time~\cite{Tekalp2014}.

NOTE:\@
tracking data to another object for example in animation. Or as it is used in the
case of this paper Video/Object Tracking.

\section{A General Approach to Motion Tracking}\label{literature_review_general_approach}
The problem of isolating moving objects of interest from a scene can
be categorised under the branch of Computer Vision known as motion
tracking.

The approaches to motion tracking can be classified broadly in two ways:
\begin{itemize}
    \item \textbf{Detection}: Assume that there is only one object
        of interest in each frame, $\mathbf{f}_k$ of an image sequence, and that
        there exists a reliable detector for the object of interest, that can
        return it's location within a particular frame of interest. In this case, the
        tracking problem can be reduced to applying the detector to each
        subsequent $\mathbf{f}_k$ of the image sequence
        $\mathbf{f}_k,\ldots,\mathbf{f}_{k+n}$ and connecting each of the
        $k$ detector responses to form a track~\cite{Forsyth2012}.

        Certain situations lend themselves well to this approach. One could
        imagine that an overhead camera in a game of billiards could easily track the
        position of the 8-ball by searching each video frame for the colour black, as
        this colour differs significantly from the green felt, and the colour of the
        other balls. However, many practical applications cannot assume reliable
        detection or single objects of interest.  Referring back to the above example,
        difficulty may arise if a player is wearing a long sleeved black shirt as the
        detector would interpret this as the ball, such a scenario may require more
        sophistication in the design of the detector.

    \item \textbf{Tracking}: Tracking is based on the tendency for an object's
        appearance to be temporally similar and exhibit coherence in motion
        between successive frames, $\mathbf{f}_k$ and $\mathbf{f}_{k+1}$ in an
        image sequence. Given a model of an object, as well a domain within
        $\mathbf{f}_k$ in which the object of interest is located. The motion
        model can be used to search for the model in the domain of interest
        within $\mathbf{f}_{k+1}$.  This is approach differs from tracking by
        detection, as it leverages dynamic inter frame information from
        $\mathbf{f}_k$ to locate the object within $\mathbf{f}_{k+1}$.
\end{itemize}

\subsection{Why object tracking?}
Given that object detection can be used to track an object by way of repeated
detection, why the emphasis emphasis on the concept object tracking?

Object Detection in the context of image sequences can
take on two forms. Assuming an initial algorithm generates regions of interest
within $\mathbf{f}_k$. For each region of interest, we can evaluate whether an
object of interest is present by feature extraction and comparison to a known
model, by way of a similarity measure or applying a trained classifier to the
feature vector. Note that this first approach makes no use of the available
temporal information between adjacent frames within the image sequence. 

The second form of Object Detection leverages temporal difference
between adjacent frames, $\mathbf{f}_k$ and $\mathbf{f}_{k+1}$ within an image
sequence to detect objects. This can also be referred to as Motion Detection.  
Regardless of the variant, detection is a key initial step to any motion
tracking algorithm.

There are inherent limitations to detection based motion tracking. With
traditional approaches detection tends to be computationally expensive.

\section{Object Modelling}
Object Modelling refers to the process of defining an internal representation of
an Object of Interest subsequent tracking.

Porikli and Yilmaz~\cite{Porikli2012} highlight three different layers in the
definition of a particular Object model. These layers are 
\begin{itemize}
    \item Model Representation,
    \item Model Descriptor 
    \item Model Low-Level Features.
\end{itemize}

\subsection{Model Representation}
The representation of an object model refers to the parametric or non-parametric
spacial definition of the object within a particular frame. Notable examples of
representations include

\subsubsection{Point and region}
This is an example of a parameterised representation. In a point and region
representation, an object is defined by a centroid and a predefined shape such
as a rectangle or an ellipse.

\subsubsection{Silhouette representation}
This is a more complex non-parameterised representation. Here, the object of
interest is defined by it's silhouette-the shape inscribed by it's outline.

There are yet more complex model representations, some of which further segment
the object of interest into smaller subregions such as with part-based
representations, for example splitting up an animal object into it's constituent
limbs. The choice of representation is dependent on the problem to which motion
tracking is to be applied~\cite{Porikli2012}.

\subsection{Model Descriptor}
Model descriptors are derived mathematically from the region of interest defined
by the chosen model representation. An effective descriptor is one which allows
for good discrimination between different objects or regions they
describe. Discrimination is characterised by a similarity measure that
facilitates a quantitative assessment of how similar a particular candidate
descriptor in $\mathbf{f}_{k}$ matches it's model descriptor.
Some common descriptors are

\subsubsection{Template}

\subsubsection{Density Estimates}
Density estimates as model descriptors can be Parametric or Non-Parametric. They
estimate the probability density function based on an underlying low-level
feature within the relevant model representation.
Common low-level features used include; colour-space values, spacial and
temporal intra- and inter-frame derivatives within a given colour-space etc.

\subsection{Model Low-Level Features}
Feature selection is a very relevant step in the development of computer vision
applications. The extent to how discriminative a particular feature is is
largely dependent on the scenarios. Hence many solutions employ a
variety of features to encode into the model robustness across multiple
scenarios. The most common visual low-level features are.

\subsubsection{colour}

\subsubsection{texture}

\subsubsection{optical flow}



The relevant literature details numerous approaches to each of the above
subtasks of the overall motion tracking problem. Before elaborating on these
approaches, it is necessary to have an understanding of challenges faced in
the practical implementations of these subtasks.

\section{Challenges faced by Computer Vision}\label{literature_review_challenges}
Addressed here are several challenges faced when developing Computer
Vision Algorithms to solve the tasks of Object Detection and Motion Tracking. 
These ideas will be necessary in assessing which approaches to focus on within
this study, in line with the objectives outlined in Section~\ref{introduction_objectives}.

The challenges are conveniently expressed in
Table~\ref{tbl:computer_vision_challenges}. Experts of image sequences 
from the VOT2017~\cite{VOT_TPAMI} datasets are used to illustrate the various
challenges.

\begin{longtable}{p{5cm}  p{8cm}}
    \hline
    \textbf{Challenge} & \textbf{Description} \\ 
    \hline\hline
    
    1. Occlusion
    & 
    Objects of interest may only be partially visible in a
    given frame, ideally we want an algorithm to still detect or
    track an object despite this.
    \\ \bottomrule
    
    2. Illumination
    &
    Algorithm performance can be significantly degraded by changes
    in lighting, if their implementation does not take this into
    account.
    \\ \bottomrule
    
    3. Ego Motion
    &
    If image sequences are not recorded by a stationary observer,
    the camera motion has to be accounted for by the algorithm.
    \\ \bottomrule
    
    4. Deformation
    &
    between frame's. For example, observing a bird flying. In this
    case an algorithm may fail if it assumes a consistent shape for
    the object of interest.
    \\ \bottomrule

    5. Orientation
    &
    Objects look different according to their orientation relative
    to the camera.
    \\ \bottomrule

    6. Scale
    &
    An object of interest may change it's scale within an image
    sequence, depending on whether it is moving away from or towards
    the observer.
    \\ \bottomrule

    7. Speed 
    &
    Many algorithms operates on the assumption of small
    changes of the position and appearance of an object between
    subsequent frames. An object moving at a high enough velocity
    affect the performance of algorithms that are not designed to
    deal with this.
    \\ \bottomrule
  
    8. Track Overlap
    &
    This is a situation in which two objects in motion cross paths, it can lead
    to occlusion of the object of interest depending on their proximity to the
    observer of the scene.
    \\ \bottomrule

    \caption{Table illustrating Computer Vision Challenges}
\end{longtable}\label{tbl:computer_vision_challenges}





\section{Object Detection}
Object detection generally refers to the identification of objects within
an image. It is thereby a segmentation of the image into foreground
and background with foreground containing the object (s) of interest for subsequent
operations.

In the context of motion tracking, the problem of object detection is a key
step. It becomes narrowed down to the more specific task of motion segmentation.
The literature details several approaches to this task, summarised in 
Figure~\ref{literature_review_motion_detection_taxonomy} below.

\Figure[width=0.7\columnwidth]{Taxonomy of Motion Detection Approaches}{literaturereview_taxonomy_motion_detection}

*Explain Diagram

\section{Object Tracking}
Moving objects in a video have a state. A state can be directly observable such as a
particular object's colour features, or position in a frame. A state can also be a function
of observable states such as the object's velocity.

In Object Tracking, the state we are interested in is the position of an object
of interest in successive frames of video. We may use states or derivatives of
them to reliably determine this \cite{Forsyth2012}.

The literature states several approaches to the motion tracking problem. The can
conveniently be expressed in the following taxonomy, Figure~\ref{fig:motion_tracking_taxonomy},
adapted from Prajapati~\cite{Prajapati2015}.

\Figure[width=0.7\columnwidth]{Taxonomy of Motion Tracking Approaches}{literaturereview_taxonomy_motion_tracking}

*Explain Higher Level*

\subsubsection{Simple Template Matching}
Template Matching is a high-level Machine Vision operation that aims to determine
what elements of a given image match a specified template of the element of
interest.
Formally we have a source image, $I$ in which we would like to obtain matches to
an element of interest defined by a template image $T$.

\subsubsection{Mean-Shift}\label{literature_review_mean_shift}


\section{GUI Design}

\section{Software}

\subsection{Relevant Languages}

\subsection{Relevant Libraries}




