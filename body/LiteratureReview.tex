\chapter{Literature Review}\label{chapter_literature_review}
This Literature Review is structured as follows: It begins by establishing key
definitions necessary for navigating the literature.

The review outlines a general framework to solving the motion tracking
problem in terms of two approaches of Detection and Tracking. It then details
the subtasks relevant to these approaches.

\section{Key Terms}
The field of Computer Vision is riddled with similar sounding terms whose intent
can easily get muddled during the discussion of concepts. The purpose of this section
is to establish the intent of these keywords as they are used later in this
review.

Detailed below are definitions of important terms within the field of computer
vision, some of which are used rather loosely within the literature or are
despite seeming interchangeable may refer to different concepts. 

\textbf{Image Segmentation}: This is the process of partitioning an image into multiple
sets of pixels with the goal of making the image easier to analyse, it typically
involves finding boundary lines of objects. 

\textbf{Motion Segmentation}: Motion Segmentation refers to the labelling of pixels that
are associated with each independently moving 3D object in sequence of images
that can feature multiple motions.~\cite{Tekalp2014}

\textbf{Object Recognition}: This a method by which Given an image, the objects within
said image are detected and classified as one of a set of predefined Object
Category i.e.\ ``What Object (s) are in this frame?''

\textbf{Object Detection}: Object Detection can be seen as targeted recognition, it deals
with finding instances of objects within a given image or video frame, this
technique usually makes use of extracted features and learning algorithms to
recognize instances of an object category.  I.e. “Where is this particular
object in this frame?”

\textbf{Object Classification}: This is a broader treatments than Detection, where a
system is not only able to identify objects within images but can differentiate
between and label different classes of objects within the image. 

\textbf{Optical Flow}: refers to the pattern of apparent motion of objects, surfaces and
edges in a visual scene caused by the relative motion between an observer and a
scene. Optic flow fields can be used to structure or segment a scene~\cite{Forsyth2012}.

\textbf{Optical Flow Segmentation}: Refers to the grouping together of optical flow
vectors that are associated with the same 3-D motion or structure. This problem
is identical to motion segmentation provided a dense optical flow field~\cite{Tekalp2014}.

\textbf{Object Tracking}: Object tracking also known as Video Tracking refers to the use
of sensor measurements to determine the location, path and characteristics of
objects of interest~\cite{Challa2011}.
 
\textbf{Motion Estimation}: A fundamental problem in video processing that is concerned
with determining motion vectors that describe the transformation of one 2D
image to another over time~\cite{Tekalp2014}.

NOTE:\@
tracking data to another object for example in animation. Or as it is used in the
case of this paper Video/Object Tracking.

\section{Approaches to Motion Tracking}\label{literature_review_general_approach}
The problem of isolating moving objects of interest from a scene can
be categorised under the branch of Computer Vision known as motion
tracking.

The approaches to tracking objects of interest can be classified broadly in two ways:
\begin{itemize}
    \item \textbf{Object Detection}: Assume that there is only one object
        of interest in each frame, $\mathbf{f}_k$ of an image sequence, and that
        there exists a reliable detector for the object of interest, that can
        return it's location within a particular frame of interest. In this case, the
        tracking problem can be reduced to applying the detector to each
        subsequent $\mathbf{f}_k$ of the image sequence
        $\mathbf{f}_{k+1},\ldots,\mathbf{f}_{k+n}$ and connecting each of the
        $k$ detector responses to form a track~\cite{Forsyth2012}.

        Certain situations lend themselves well to this approach. One could
        imagine that an overhead camera in a game of billiards could easily track the
        position of the 8-ball by searching each video frame for the colour black, as
        this colour differs significantly from the green felt, and the colour of the
        other balls. However, many practical applications cannot assume reliable
        detection or single objects of interest.  Referring back to the above example,
        difficulty may arise if a player is wearing a long sleeved black shirt as the
        detector would interpret this as the ball, such a scenario may require more
        sophistication in the design of the detector.

    \item \textbf{Motion Tracking}: Tracking is based on the tendency for an object's
        appearance to be temporally similar and exhibit coherence in motion
        between successive frames, $\mathbf{f}_k$ and $\mathbf{f}_{k+1}$ in an
        image sequence. Given a model of an object, as well a domain within
        $\mathbf{f}_k$ in which the object of interest is located. The motion
        model can be used to search for the model in the domain of interest
        within $\mathbf{f}_{k+1}$.  This is approach differs from tracking by
        detection, as it leverages dynamic inter frame information from
        $\mathbf{f}_k$ to locate the object within $\mathbf{f}_{k+1}$.
\end{itemize}

\section{Challenges to Motion Tracking}\label{literature_review_challenges}
Addressed here are several challenges faced when developing computer
vision algorithms to solve the tasks of object detection and motion tracking. 
These ideas will be necessary in assessing which approaches to focus on within
this study, in line with the objectives outlined in Section~\ref{introduction_objectives}.

The challenges are conveniently expressed in
Table~\ref{tbl:computer_vision_challenges}. Illustrations of these challenges
are presented in Chapter~\ref{chapter_results} of this report.

\begin{longtable}{p{5cm}  p{8cm}}
    \hline
    \textbf{Challenge} & \textbf{Description} \\ 
    \hline\hline
    
    1. Occlusion
    & 
    Objects of interest may only be partially visible in a
    given frame, ideally we want an algorithm to still detect or
    track an object despite this.
    \\ \bottomrule
    
    2. Illumination
    &
    Algorithm performance can be significantly degraded by changes
    in lighting, if their implementation does not take this into
    account.
    \\ \bottomrule
    
    3. Ego Motion
    &
    If image sequences are not recorded by a stationary observer,
    the camera motion has to be accounted for by the algorithm.
    \\ \bottomrule
    
    4. Deformation
    &
    between frame's. For example, observing a bird flying. In this
    case an algorithm may fail if it assumes a consistent shape for
    the object of interest.
    \\ \bottomrule

    5. Orientation
    &
    Objects look different according to their orientation relative
    to the camera.
    \\ \bottomrule

    6. Scale
    &
    An object of interest may change it's scale within an image
    sequence, depending on whether it is moving away from or towards
    the observer.
    \\ \bottomrule

    7. Speed 
    &
    Many algorithms operates on the assumption of small
    changes of the position and appearance of an object between
    subsequent frames. An object moving at a high enough velocity
    affect the performance of algorithms that are not designed to
    deal with this.
    \\ \bottomrule
  
    8. Track Overlap
    &
    This is a situation in which two objects in motion cross paths, it can lead
    to occlusion of the object of interest depending on their proximity to the
    observer of the scene.
    \\ \bottomrule

    \caption{Table illustrating Computer Vision Challenges}
\end{longtable}\label{tbl:computer_vision_challenges}

\section{General Framework of Motion Tracking}
Porikli and Yilmaz~\cite{Porikli2012} defines a sufficiently general but
comprehensive video analytic framework that outlines the fundamentals of video
analytics based on detection and tracking. 

The framework consists of three stages
\begin{itemize}
    \item \textbf{Detection}:
    \item \textbf{Modelling}: 
    \item \textbf{Tracking}:
\end{itemize}

There are various approaches to each stage that are subsequently addressed.

\section{Object Detection}\label{literature_review_object_detection}
Object Detection is the initial step to any motion tracking algorithm. It can
either be change-based or classification-based, it is important to distinguish
between these two.

In classification-based object detection, no temporal information is directly
used. Assuming an initial algorithm generate regions of interest within a frame, this
could very well be regions identified by a change detector, edge detector or a
simple sliding window. A representative set of features is extracted for each
region of interest. For each region of interest, comparison to a known model
feature using a similarity measure can determine the likelihood of an object of
interest being present. 
This could be by comparison to a set of template models, or more generally the
output of a trained classifier.

Change-based object detection leverages temporal differences between adjacent frames
$\mathbf{f}_k$ and $\mathbf{f}_{k+1}$ within an image sequence to detect
changes. 

\subsection{Why motion tracking?}
Given that object detection can be used to track an object by way of repeated
detection, why the emphasis on the concept motion tracking?

Object and there are inherent limitations to tracking based purely on
detection based motion tracking as detection tends to be computationally
expensive. Classification-based detection is also limited to objects who's
models are known a priori~\cite{Forsyth2012}.
Tracking also allows for a variety approaches that address a lot of the
challenges to the tracking problem outlined in
Section~\ref{literature_review_challenges}.

Figure~\ref{literature_review_motion_detection_taxonomy} outlines some
approaches to object detection that a motion tracker implementation might take.

\Figure[width=0.7\columnwidth]{Taxonomy of Motion Detection Approaches}{literaturereview_taxonomy_motion_detection}

*Explain Diagram

\section{Object Modelling}\label{literature_review_object_modelling}
Object modelling refers to the process of defining an internal model of
an identified object of interest subsequent tracking. object modelling can be
broken down into several layers, these being:

\begin{itemize}
    \item \textbf{Model Representation}: spacial region occupied by object within a
        frame.
    \item \textbf{Model Descriptor}: Mathematical definition of an object within
        a frame. A descriptor is derived from an object's underlying low-level information over the spacial
        region it occupies defined by the object's representation.
    \item \textbf{Model Low-Level Features}: characteristics inherent to the
        observable pixels in a frame.
\end{itemize}

\subsection{Model Representation}
The representation of an object model refers to the parametric or non-parametric
spacial definition of the object within a particular frame. Notable examples of
representations include

\subsubsection{Point and region}
This is an example of a parameterised representation. In a point and region
representation, an object is defined by a centroid and a predefined shape such
as a rectangle or an ellipse.

\subsubsection{Silhouette representation}
This is a more complex non-parameterised representation. Here, the object of
interest is defined by it's silhouette-the shape inscribed by it's outline.

There are yet more complex model representations, some of which further segment
the object of interest into smaller subregions such as with part-based
representations, for example splitting up an animal object into it's constituent
limbs. The choice of representation is dependent on the problem to which motion
tracking is to be applied~\cite{Porikli2012}.

\subsection{Model Descriptor}
Model descriptors are derived mathematically from the region of interest defined
by the chosen model representation. An effective descriptor is one which allows
for good discrimination between different objects or regions they
describe. Discrimination is characterised by a similarity measure that
facilitates a quantitative assessment of how similar a particular candidate
descriptor in $\mathbf{f}_{k}$ matches it's model descriptor.
Some common descriptors are

\subsubsection{Templates}
Templates are extractions of the pixels in the region of interest of the
underlying frame. They are usually extracted according to a particular
parametric shape, but can also be defined for more complex model representations.

\subsubsection{Density Estimates}
Density estimates as model descriptors can be Parametric or Non-Parametric. They
estimate the probability density function based on an underlying low-level
feature within the relevant model representation.
Common low-level features used include; colour-space values, spacial and
temporal intra- and inter-frame derivatives within a given colour-space etc.

\subsection{Model Low-Level Features}
Feature selection is a very relevant step in the development of computer vision
applications. The extent to how discriminative a particular feature is is
largely dependent on the scenarios. Hence many solutions employ a
variety of features to encode into the model robustness across multiple
scenarios. The most common visual low-level features are.

\subsubsection{colour}
In digital images, colours can be represented in one of several colour spaces, the
most common representation is within the RGB (red, green, blue) colour space. In
the RGB colour space, each pixel is represented by integer in the range
$[0,256)$ for each channel in the colour space. e.g black = $\{0,0,0\}$.

\subsubsection{texture}
Texture measures the variation in intensity of a region within a frame. It can
be interpreted as how smooth or rough the surface of the region is.
For example rough surfaces could be characterised by large variations in pixel
values over short distances corresponding to bumps in an object, or low
variation corresponding to a smooth object with a uniform surface\cite{Porikli2012}

\subsubsection{optical flow}
Optical flow is the most general approach to motion estimation. It involves
computing an independent estimate of the motion at each pixel location for each
frame in a sequence.
The result of this is a dense field of optical flow vectors for each frame in
the relevant sequence. Each pixel's vector is characterised by an orientation.
It is easy to imagine that clusters of flow vectors of similar magnitude and
direction could correspond to moving objects between two
frames\cite{Szeliski2010}.

\section{Motion Tracking}
Once an object has been detected in an initial frame, $\mathbf{f}_k$ based on the selected
target model, motion tracking is concerned with locating object in subsequent
frames $\mathbf{f}_{k+1}$ to $\mathbf{f}_{k+n}$ within the image sequence. 

The literature states several tracking methods. These can
conveniently be expressed in the following taxonomy outlined in
Figure~\ref{fig:literaturereview_taxonomy_motion_tracking}, that was adapted
from similar presentations in~\cite{Prajapati2015, Patel2013}. The various
approaches are described in terms of the layers to object modelling described in
Section~\ref{literature_review_object_modelling}.

\Figure[width=0.7\columnwidth]{Taxonomy of Motion Tracking Approaches}{literaturereview_taxonomy_motion_tracking}

\subsection{Point tracking}


\subsection{Kernel tracking}
Kernel based tracking is usually performed by defining a kernel around an object
of interest. A kernel is usually a parametric model representation such as an
ellipse or square, that encompasses the target object. 
Variations between different kernel tracking methods lie in the chosen representations,
descriptors and their underlying low-level features~\cite{Prajapati2015}

\subsubsection{Simple template matching}
Template Matching is a pattern recognition technique in which stored patterns 
are identified within an image. It can be performed at various levels ranging
from a pixel-level to higher level comparisons of average intensity.

It is an operation that aims to determine what elements of a given image match a
specified template of the element of interest. Formally we have a source image,
$I$ in which we would like to obtain matches to an element of interest defined
by a template image $T$~\cite{Brunelli}. 
Given a target template defined in $\mathbf{f}_k$, we can track an object by 
template matching in $\mathbf{f}_{k+1},\ldots,\mathbf{f}_{k+n}$.

\subsubsection{Mean shift tracking}\label{literature_review_mean_shift}
The mean-shift tracking algorithm provides an efficient tracking approach for
target objects modelled by with density estimate descriptors such a colour or
texture histograms. 
Given the location of a detected object in $\mathbf{f}_k$, mean-shift tracking
locates an object in $\{\mathbf{f}_{k+1},\ldots,\mathbf{f}_{k+n}\}$ by way of a
non parametric gradient ascent procedure of a smooth similarity function around
the last known location of an object~\cite{Comaniciu2003}. This procedure is
extensively presented in Section~\ref{theoretical_framework_mean_shift_tracker}.

\subsection{Silhouette tracking}


\section{GUI Design}



