\chapter{Theoretical Framework}\label{chapter_theoretical_framework}

This Chapter aims to provide a comprehensive treatment of the theory necessary
for the implementation of the Overall MOT System. This is in line with the
objectives of the study outlined in Section~\ref{introduction_objectives} and
the research direction determined in reviewing the relevant literature in
Chapter~\ref{chapter_literature_review}.

In Sections~\ref{theoretical_framework_tm} and~\ref{theoretical_framework_ch} we first
explore the development of a Tracker that is based purely on Detection. We
review both correlation based and feature based approaches within field of Template Matching.

Section~\ref{theoretical_framework_ch} presents the feature based Template
Matching based on the Co-occurrence Histogram texture feature as proposed by
Chang~\cite{Chang1999}. The presentation is also supplemented treatment of the
feature extraction by Hall~\cite{Hall-beyer2018}.

Section~\ref{theoretical_framework_mean_shift_tracker} draw heavily from the
work and presentations of the by~\cite{Comaniciu2002, Comaniciu2003,
Shah2011} of the theory behind the Mean Shift Tracker. They do however take the
liberty in making some changes to the notation and order of presentation, while
adding some visual illustrations and
context to certain steps. 


\section{Template Matching}\label{theoretical_framework_tm}
The detection and recognition of objects within an image is one of the
fundamental Computer vision problems. A basic approach to this task is Template
Matching.

Template Matching is easily defined by its constituent words. A template is a
model, representation for a particular object or task and matching is
association of one or more objects by some measure of similarity. Template
Matching is a technique by which a known object represented by a template, $t$
can be localised within a larger image or frame of interest, $\mathbf{f}_k$.

\subsection{Simple template matching}\label{theoretical_framework_simple_tm}
In it's simplest form, 

\subsection{Adaptive template matching}\label{theoretical_framework_adaptive_tm}


\section{Colour co-occurrence histogram detection}\label{theoretical_framework_ch}
As presented in Sections~\ref{results_simple_template_matching} and~\ref{
results_adaptive_template_matching}, the simple and adaptive template matching
approaches showed some success when applied to the motion
tracking problem. However, the simple template tracker was unable to cope with
slight rotation of the target as the sequence progressed.
While the Adaptive variant could deal with these changes in target
orientation, it suffered the problem of ``drift'' in which the model contained
more and more noise as the sequence progressed.
Furthermore the adaptive template tracker could not cope with the problem of
occlusion.

Both variants of the simple template tracking approach suffers from poor generalisation- the
ability to adapt to unseen data. Template Matching applications often require
that templates be characterized by the joint use of geometrical and textural
information~\cite{Brunelli}, i.e.\ a feature based approach.

Furthering the concept that a reliable detector can make for a reliable tracker,
the work by Chang et al.~\cite{Chang1999}, proposed a robust template
localization approach based on the use of co-occurrence histograms as template
features. A description of the theory underlying the implementation of such a
detector follows.

\subsection{Model representation}
A colour histogram is a binned count of pixel values over a particular image
region, within a particular colour space. This entirely abstracts away the geometry of
the image.
A CH can be thought of as a colour histogram augmented with with spacial
information in addition to the frequencies of it's values in a particular colour
space.

Defined within the RGB-colour space, a CH holds the number of occurrences of
pixel pairs $\mathbf{P_i}=(R_i,G_i,B_i)$ and $\mathbf{P_j}=(R_j,G_j,B_j)$ that
are separated by a vector $\vec{v}=(\Delta{x}, \Delta{y})$ in the pixel plane.

For the CH-Detector, instead of $\vec{v}$, we use it's euclidean norm $d$ as our
augmented feature where 
$$d = \sqrt{{(\Delta{x})}^2 + {(\Delta{y})}^2}$$ 
This is a necessary step in order to reintroduce invariance to orientation for
the CH feature. 

The colours are quantized into a set $C$ of $n_c$ colour levels, where 
$C = \{c_1,c_2,\dots,c_{n_c}\}$. Likewise the distances are quantized into
a set $D$ consisting of $n_d$ distances, where $D = \{\left[ 0,1
\right),\left[ 1,2\right), \dots , \left[ n_{d-1},n_{d} \right)\}$.

The colour quantization is conveniently achieved by application of the kmeans clustering
algorithm to the set of pixels in our region of interest, characterised by their
euclidean distance within the RGB-colour space. The number of clusters is
specified as $n_c$. (see Appendix~\ref{appendix_A} for a review of the kmeans
algorithm). 

With this we can settle on a definition for our CH feature as
$$\text{CH} = \{C_d\}_{d \in D}$$ 
where $C_d \in \mathbb{Z}^{n_c \times n_c}$ index the
frequencies/counts of the quantized source and destination pixels within an
image at each distance $d \in D$. Row and column indices of
$C_d$, $(i,j)$ represent source and destination bins respectively.  


\subsection{Similarity measure}\label{theoretical_framework_intersection}
The problem is here is fundamentally similar to that in the Simple Template
Matching case. Given a template we would like to locate it within a given frame.
This is also done by the sliding window approach.

The CH-Detector involves the added step of feature extraction. In a basic
implementation, we initially extract our model $CH_m$ from the known template.
With the sliding window approach, candidate models $CH_c$ are extracted and
assessed for their similarity to $CH_m$. The region in which the $CH_c$ most
similar to the $CH_m$ is regarded as the match. 

The similarity measure used by \cite{Chang1999} is the 
Intersection between $CH_m$ and a potential $CH_c$. The main motivation behind
this is that the Intersection is relatively robust against changes in the
dimension of the sliding window.
We can thereby avoid a computationally expensive exhaustive search by initially
searching with a larger search window, and hierarchically refining the search
window.

The Intersection between a model and candidate histogram is defined as
\begin{equation}\label{eqn:intersection}
    I_{cm} = \sum_{k=0}^{n_d}\sum_{i=0}^{n_c}\sum_{j=0}^{n_c} = min[(CH_m(k,i,j),CH_c(k,i,j))]
\end{equation}
where:
\begin{itemize}
    \item $k$ iterates the $n_d$ distances of the CH lattice.
    \item $i,j$ iterates the source and destination quantized pixel countes level respectively over the
        region of interest.
\end{itemize}
Chang et al~\cite{Chang1999} expresses the Intersection as a measure of how well the
candidate accounts for the model CH\@. In taking the minimum between the two
for each quantization level and distance two things are occurring. Firstly if a
candidate has high counts for certain quantizations, the $I_{cm}$ ignores these
by keeping the model counts. Conversely if a candidate has low counts for a
model's high quantized counts they are taken pulling the $I_{cm}$ down.

The similarity is measured on a scale from 0 to to the highest possible value of
$I_{cm}$ which is a perfect match obtained by the auto-intersection of our
model $I_{mm}$.
The criteria for finding an object is therefore presented by a Threshold, $\tau$ which
is a fraction, $\alpha$ of the maximum possible Intersection $I_{mm}$
$$\tau=\alpha I_{mm}$$


\subsection{Co-occurrence histogram detector algorithm}

\Figure[width=0.65\columnwidth]{Flow chart of co-occurrence histogram localisation algorithm for $\mathbf{f}_k$}{design_algorithm_CH}

The general search algorithm used is shown in
Figure~\ref{fig:design_algorithm_CH}. Given the model CH $q$ and number of
levels in the search $n$, The localisation algorithm specifies the search window
size $w$ of dimensions scaled by $sf_n$ for a particular $n$. The Search
Algorithm then convolves this search window with the image with a specific step
and at each overlap computes the Intersection of the candidate and the known
model CH's $q$.  At the end of a Search, the best match of a particular level
$n$ is then fed back to the lower level for a more refined search, for which the
localisation algorithm defines a refined search window. At the lowest search
level where $n=0$, the object is located.

\section{Mean shift tracker}\label{theoretical_framework_mean_shift_tracker}
As summarised in Section~\ref{literature_review_mean_shift}, the mean shift tracker was
first proposed by Commaniciu et al~\cite{Comaniciu2003}. It is addresses the
visual tracking problem of target representation and localisation.

The idea of this tracker is that spatially masking an object with an
isotropic kernel allows for the definition of a spatially-smooth similarity
function relating a target and potential candidate model in a particular feature
space. This smooth similarity lends itself to gradient-based optimization,
thereby enabling efficient target localisation in subsequent frames.
A description of theory underlying the mean shift tracker follows:

\subsection{Mean shift}\label{theoretical_framework_mean_shift}
The Mean Shift Method is an algorithm that is capable of finding the modes of a 
non-parametric distribution by use of samples of the unknown underlying distribution. 

\subsection{Model representation}
To represent a particular target or candidate Model, a suitable feature space
must be chosen. This choice could be anything from greyscale intensity, RGB
intensity or Texture of the particular object. Objects are geometrically described 
as elliptical sub-domains $\mathbf{D}$ of a frame, with dimensions $h_x$ and
$h_y$, centred at pixel location $\mathbf{c}=(x,y)$ within the frame. (see
Figure~\ref{fig:TF_epanechnikov_kernel} for an illustration)

Target and candidate models are defined as PDF's within the chosen feature
space defined over $\mathbf{D}$. In the initial frame, $\mathbf{f}_k$, a target
model of the object to be tracked located at $\mathbf{c_0}$ is defined by it's
pdf, $\hat{q}(\mathbf{c_0})$. In the subsequent frame, $\mathbf{f}_{k+1}$, a
candidate model is defined with pdf $\hat{p}(\mathbf{c_0})$ starting at the last
known object position $\mathbf{c_0}$. 

Both PDF's $\hat{q}(\mathbf{c_0})$ and $\hat{p}(\mathbf{c_0})$ are estimated from
their respective frames $\mathbf{f}_k$ and $\mathbf{f}_{k+1}$ using $m$-bin
weighted histograms within the relevant feature space. The weighting is achieved
by use of a Kernel. 

\subsection{Kernel function}
A Kernel is a weighting function employed in non-parametric density estimation.
The recommended kernel function for the mean shift tracker is that of the
Epanechnikov kernel~\cite{Comaniciu2002}.
The Epanechnikov Kernel has the following general equation:
\begin{equation}
    \kappa(x)=
    \begin{cases}
        \frac{1}{2c_d}(d+2)(1-x), & |x|<1 \\
        0, & \text{otherwise}
    \end{cases}
\end{equation}
where:
\begin{itemize}
   \item $d$: the dimensions of the kernel
   \item $c_d$: the area of a unit circle in the dimension $d$
   \item $x$: the square of the Euclidian distance of the normalized pixel in
       $\mathbf{n_x}$
\end{itemize}

Of interest is the 2D variant of this equation given that the spatial smoothing
is occurring in the 2D $(x,y)$ pixel plane. Thus we choose $d=2$ and $c_d=\pi$.
As the kernel profile does not vary for a particular region of interest, it is
computationally convenient to pre-calculate its values for use in the subsequent
derivation of the histogram.

The calculation of the Epanechnikov Kernel Matrix for a particular $\mathbf{D}$ is
illustrated in Figure~\ref{fig:TF_epanechnikov_kernel}.

\Figure[width=0.8\columnwidth]{Illustration of Epanechnikov Matrix Derivation}{TF_epanechnikov_kernel}

The pixels in the elliptical mask of the target/candidate are normalized to a unit
circle by scaling the ellipse by $h_y$ and $h_x$ (which are the major and minor
axes size of the ellipse). Each pixel is then assigned a weight according to the
value of the 2D Epanechnikov Kernel profile for said pixel's normalized euclidean
distance from the unit circle centre as illustrated in Figure~\ref{fig:TF_epanechnikov_kernel}.

These weights derived from the kernel profile are subsequently used to scale
pixel counts when creating m-bin histogram. The monotonically decreasing Kernel
assigns higher weight to pixels at the centre of $\mathbf{D}$ while giving lower
weighting to pixels further away from the domain center $\mathbf{c}$. 
This is desirable because not only does it enable the development of a spatially
smooth similarity function as addressed in Section~\ref{maximising_bhat} but it
also encodes into our model robustness to the effects of occlusions at the
periphery of the object domain.

\subsection{PDF development}
Computing the PDF then occurs according to the following equation:
\begin{equation}\label{eqn:mean_shift_histogram}
    \text{pdf}(u)=C\sum_{i=1}^{n_x}\kappa({\parallel{\mathbf{x_i}^*}-{\mathbf{c_0}^*}\parallel}^2)\delta[b(\mathbf{x_i}^*)-u]    
\end{equation}
\begin{equation}\label{eqn:histogram_normalisation}
    C=\frac{1}{\sum_{i=1}^{n_x}\kappa({\parallel\mathbf{x_i}^*\parallel}^2)}
\end{equation}
where:
\begin{itemize}
    \item $\mathbf{x_i}^*$: normalised pixel 
    \item $n_x$: normalised pixel domain.
    \item $\parallel{\mathbf{x_i}}^*-{\mathbf{x_0}}^*\parallel$: The euclidean
        norm between $\mathbf{x_i}^*$ and ${\mathbf{x_0}}^*$ squared which
        reduces to $\parallel{\mathbf{x_i}}^*\parallel$ for $\mathbf{c_0} = 0$. 
    \item $\kappa({\parallel{\mathbf{x_i}}^*\parallel}^2)$: Kernel function that
        defining weights based on $\parallel{\mathbf{x_i}}^*\parallel^2$.
    \item $b({\mathbf{x_i}}^*)$: A function which maps the normalized pixel
        values in $\mathbf{n_x}$ to their RGB pixel value in $\mathbf{D}$.
    \item $\delta[b({\mathbf{x_i}}^*)-u]$: Abstraction to say only evaluate for bin u.
    \item $C$: Normalisation constant to ensure histogram satisfies Equation~\ref{eqn:histogram_normalisation}.
\end{itemize}

Depending on the number of features used computational complexity of calculating
the histogram and space needed for storing the histogram goes up by a factor of
$m$, as we are increasing the dimensionality of our feature space by 1 for each
additional feature thus incrementing the size by a factor of $m$-bins.

The models for the target and candidate are summarised below:
\[\text{target model:}\hat{q}(\mathbf{c_0}) = {\{\hat{q}_u(\mathbf{c_0})\}}_{u=1,\ldots,m}\]
\[\text{candidate model:}\hat{p}(\mathbf{c_i}) = {\{\hat{p}_u(\mathbf{c_i})\}}_{u=1,\ldots,m}\]

All model/candidate histograms are normalized. Therefore, a general m-bin
histogram, $\hat{p}(\mathbf{c_i})$ defined over spacial sub-domain, $\mathbf{D}$
within a frame $\mathbf{f}_k$, satisfies the following normalisation condition:
\[\sum_{u=1}^{m}\hat{p}_u = 1\]

\subsection{Similarity measure}
Given a target model's pdf $\hat{q}(\mathbf{c_0})$, and a candidate pdf,
$\hat{p}(\mathbf{c_i})$, in a subsequent frame.  The function $\rho$ returns a
metric describing how strongly a target $\hat{p}(\mathbf{c_i})$ resembles
$\hat{q}(\mathbf{c_0})$ i.e.
$\rho_i \equiv \rho[\hat{q}(\mathbf{c_0}),\hat{p}(\mathbf{c_i})]$

As per~\cite{Comaniciu2003} we make use of the Bhattacharyya coefficient, $BC$ as our
measure of similarity measure $\rho$ between the target and candidate pdfs. The
Bhattacharyya coefficient is defined by the following equation:
\begin{equation}\label{eqn:bhattacharyya}
    BC = \rho[\hat{q}(\mathbf{c_0}),\hat{p}(\mathbf{c_i})]=\sum_{u=1}^{m}\sqrt{\hat{p}_u(\mathbf{c_i})\hat{q}_u(\mathbf{c_0})}
\end{equation}

\subsection{Maximising the Bhattacharyya coefficient}\label{maximising_bhat}
Since a monotonically decreasing kernel weighting function is applied in the
derivation of our feature histograms, the similarity function $\rho$ defined by the
Bhattacharyya Coefficient is smooth over the (x,y) domain.

As stated, we begin with a target model $\hat{q}(\mathbf{c_0})$ in frame k and a candidate
model at the same location $\hat{p}(c_0)$ in $\mathbf{f}_{k+1}$. We compute
$\rho_0=\rho[\hat{q}(\mathbf{c_0})\hat{p}(\mathbf{c_0})]$ 

Since $\rho$ is spatially smooth, we can apply optimization. This involves
taking the first order Taylor Series approximation of $\rho$ around our initial
estimate $\mathbf{c_0}$. 
\begin{equation}\label{eqn:rho_taylor}
    \rho[\hat{q}(\mathbf{c_0})\hat{p}(\mathbf{c_i})]=\rho[\hat{q}(\mathbf{c_0})\hat{p}(\mathbf{c_0})]+\frac{1}{2}\sum_{i=1}^{m}\hat{p}(\mathbf{c_i})\sqrt{\frac{\hat{q}_u}{\hat{p}_u(\mathbf{c_0})}}
\end{equation}

This is a valid approximation for small changes to $\hat{p}(\mathbf{c_i})$
which usually holds between adjacent frames in the video sequence. 

The first term on the right hand side is a constant, by maximising the second
term 
\[\frac{1}{2}\sum_{i=1}^{m}\hat{p}(\mathbf{c_i})\sqrt{\frac{\hat{q}_u}{\hat{p}_u(\mathbf{c_0})}}\]
We maximise $\rho$ and in doing so we locate our object in frame k+1.

By substitution of Equation~\ref{eqn:mean_shift_histogram} for $\hat{p}(\mathbf{c_i})$ in
the second term of Equation~\ref{eqn:rho_taylor} it can be observed that is that the
second term is the density estimate of the underlying pdf at center
$\mathbf{c_i}$ in $\mathbf{f}_{k+1}$ with pixels weighted according to the
following equation:

\[\rho_0=\rho[\hat{p}(\mathbf{c_0})]=\frac{C}{2}\sum_{i=1}^{m}w_i\kappa({\parallel{\mathbf{x_i}^*}-{\mathbf{c_0}^*}\parallel}^2)\]

\begin{equation}\label{eqn:mean_shift_weights}
    w_i(\mathbf{c_0})=\sum_{u=1}^{m}\sqrt{\frac{\hat{q}_u}{\hat{p}_u(\mathbf{c_0})}}\delta[b(\mathbf{x_i}^*-u)]
\end{equation}

Using the mean shift procedure outlined in Section~\ref{theoretical_framework_mean_shift}, $\rho$ can be maximised thus locating
our object in frame $\mathbf{f}_{k+1}$.

\subsection{Mean shift tracker algorithm}\label{theoretical_framework_mean_shift_algorithm}
The general algorithm for mean shift tracking between two frames is defined by
flow chart in Figure~\ref{fig:design_mean_shift_tracking_algorithm}. This algorithm follows
the optimized practical implementation expanded on by Comaniciu~\cite{Comaniciu2003}. 

\Figure[width=0.75\columnwidth]{Flow chart of mean shift tracking algorithm between two frames $\mathbf{f}_k$ and $\mathbf{f}_{k+1}$}{design_mean_shift_tracking_algorithm}

The preceding theory for the basis for the implementation of the mean shift
Tracker implemented in Chapter~\ref{chapter_implementation}.
